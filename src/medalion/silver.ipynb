{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233ebcbc-6316-4770-a91e-35d28be0cba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, LongType, TimestampType \n",
    "from pyspark.sql.functions import concat_ws, col, regexp_replace, when, lit, desc, max, to_date, regexp_extract, count, to_timestamp, date_format, udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c616d7-13bf-4033-b05b-0a015fd92726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Admin/src/spark_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652d895b-9b64-486a-a3ac-0aa476b901d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_ultima_partition(nome_tabela, coluna_particao):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"id_envio\", StringType(), nullable=True),\n",
    "        StructField(\"corredor_de_armazenagem\", StringType(), nullable=True),\n",
    "        StructField(\"metodo_de_envio\", StringType(), nullable=True),\n",
    "        StructField(\"ligações_do_cliente\", StringType(), nullable=True),\n",
    "        StructField(\"avaliação_do_cliente\", StringType(), nullable=True),\n",
    "        StructField(\"preço\", StringType(), nullable=True),\n",
    "        StructField(\"qtd_itens\", StringType(), nullable=True),\n",
    "        StructField(\"importancia\", StringType(), nullable=True),\n",
    "        StructField(\"genero\", StringType(), nullable=True),\n",
    "        StructField(\"desconto\", StringType(), nullable=True),\n",
    "        StructField(\"peso_g\", StringType(), nullable=True),\n",
    "        StructField(\"Chegou_no_tempo\", StringType(), nullable=True),\n",
    "        StructField(\"Destino\", StringType(), nullable=True),\n",
    "        StructField(\"DataEnvio\", StringType(), nullable=True),\n",
    "        StructField(\"dataEntrega\", StringType(), nullable=True),\n",
    "        StructField(\"avaliacaoEntrega\", StringType(), nullable=True),\n",
    "        StructField(\"data_ref\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    print(f\"Tentando ler arquivo Delta de: {nome_tabela}\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    dbutils = DBUtils(spark) \n",
    "\n",
    "    # 1- Listando arquivos Delta no Volume Bronze\n",
    "    print(f\"\\n--- Verificação do caminho Volume Bronze ---\")\n",
    "    try:\n",
    "        dbutils.fs.ls(nome_tabela)\n",
    "        print(f\"STATUS OK - Caminho '{nome_tabela}' existe. Tentando carregar como Delta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - O caminho '{nome_tabela}' não existe ou não é acessível. Detalhe: {e}. Gerando um DataFrame vazio.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # 2-  Recuperar partições existentes\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela)\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - Erro ao acessar a tabela '{nome_tabela}': {e}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        ultima_particao_df = df.select(max(coluna_particao).alias(\"ultima_particao\"))\n",
    "        \n",
    "        ultima_particao = (\n",
    "            ultima_particao_df.first()[\"ultima_particao\"]\n",
    "            if ultima_particao_df.first()\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"STATUS ALERTA - Erro ao obter última partição da tabela '{nome_tabela}': {e}\")\n",
    "\n",
    "    # 3- Tentando ler o arquivo Delta do Volume Bronze\n",
    "    print(f\"\\n--- Verificação da leitura do arquivo Delta ---\")\n",
    "    try:\n",
    "\n",
    "        filtro = f\"{coluna_particao} = '{ultima_particao}'\"\n",
    "\n",
    "        df = spark.read.format(\"delta\").load(nome_tabela) \\\n",
    "                .where(filtro)\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' carregada com sucesso pela partição: {ultima_particao}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Em caso de erro, retorne um dataframe vazio com o schema definido anteriormente\n",
    "        print(f\"STATUS ALERTA - Erro ao carregar a tabela Delta '{nome_tabela}'. Provavelmente não é uma tabela Delta válida ou não contém dados. Detalhe do erro: {e}\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # 4- Verificando se o dataframe está vazio\n",
    "    # Em caso positivo, retorne um dataframe vazio com o schema definido anteriormente\n",
    "    print(f\"\\n--- Verificação se arquivo Delta está vazio ---\")\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"STATUS ALERTA - Tabela '{nome_tabela}' foi carregada como Delta VÁLIDA, mas está completamente vazia. Retornando DataFrame vazio com o schema definido.\")\n",
    "        print(\"\\nRetornando um dataframe vazio!\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    else:\n",
    "        print(f\"STATUS OK - Tabela Delta '{nome_tabela}' não está vazio.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Caminho para a external location do diretório bronze\n",
    "bronze_path = f\"/Volumes/grainlogistic/grain/bronze/\"\n",
    "\n",
    "# Lendo arquivo Delta do diretório bronze pela ultima partição\n",
    "df = carregando_ultima_partition(bronze_path, \"data_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0632df73-f31a-41c7-bd4b-53d70050f87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_not_null_value(df, coluna):\n",
    "\n",
    "    print(f\"Iniciando o filtro de valores vazios na coluna: {coluna}\")\n",
    "\n",
    "    dffiltered = df.filter(col(coluna).isNotNull())\n",
    "\n",
    "    qtddotal = df.count()\n",
    "    qtdnotnull = df.filter(col(coluna).isNotNull()).count()\n",
    "    qtdnull = df.filter(col(coluna).isNull()).count()\n",
    "\n",
    "    print(f\"\\nDataframe filtrado, numero de linhas: {qtdnotnull}\")\n",
    "\n",
    "    assert qtddotal == (qtdnull + qtdnotnull), \\\n",
    "    f\"\\nErro na contagem: O total de linhas ({qtddotal}) não é igual à soma de nulos ({qtdnull}) e não nulos ({qtdnotnull}) para a coluna '{coluna}'.\"\n",
    "\n",
    "    print(f\"\\nFiltro ralizado com sucesso\")\n",
    "    print(f\"\\ndf origem {qtddotal} linhas = df filtrado {qtdnotnull} linhas + df não filtrado {qtdnull} linhas\")\n",
    "\n",
    "    return dffiltered\n",
    "\n",
    "dffiltered = filter_not_null_value(df, \"id_envio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144b229c-abe8-4bf6-a51e-b196e3de4ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def define_numeric_columns(df):\n",
    "\n",
    "    print(f\"Iniciando conversão de colunas com padrões inteiros/decimais\")\n",
    "\n",
    "    # Regex para identificar valores inteiras e monetários\n",
    "    regex_inteiro = re.compile(r\"^\\d+$\")\n",
    "    regex_decimal = re.compile(r\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\")\n",
    "\n",
    "    # Obtendo colunas de tipo string\n",
    "    colunas_string = [coluna for coluna, dtype in df.dtypes if dtype == \"string\"]\n",
    "    colunas_inteiras = []\n",
    "    colunas_decimais = []\n",
    "\n",
    "    # Identifica colunas com valores inteiras e monetários\n",
    "    for coluna in colunas_string:\n",
    "        df_sem_nulos = df.filter(col(coluna).isNotNull())\n",
    "        valores_amostra = df_sem_nulos.select(coluna).rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        if any(bool(regex_inteiro.match(str(valor))) for valor in valores_amostra):\n",
    "            print(f\"\\nA coluna '{coluna}' contém valores no formato inteiro.\")\n",
    "            colunas_inteiras.append(coluna)\n",
    "        elif any(bool(regex_decimal.match(str(valor))) for valor in valores_amostra):\n",
    "            print(f\"\\nA coluna '{coluna}' contém valores no formato decimal.\\n\")\n",
    "            colunas_decimais.append(coluna)\n",
    "\n",
    "        else:\n",
    "            print(f\"\\nA coluna '{coluna}' não contém valores no formato inteiro ou decimal.\\n\")\n",
    "\n",
    "    # Aplica a conversão para valores percentuais\n",
    "    for coluna in colunas_inteiras:\n",
    "        df = df.withColumn(coluna, col(coluna).cast(\"long\"))\n",
    "\n",
    "        print(f\"STATUS OK - Coluna {coluna} convertida com sucesso para tipo 'inteiro'.\")\n",
    "\n",
    "    # Aplica a conversão para valores monetários\n",
    "    for coluna in colunas_decimais:\n",
    "        df = df.withColumn(coluna, col(coluna).cast(\"double\"))\n",
    "\n",
    "        print(f\"STATUS OK - Coluna {coluna} convertida com sucesso para tipo 'decimal'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_numeric = define_numeric_columns(dffiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9bc06c-334a-4959-9718-4ef546fb8ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def define_data_columns(df):\n",
    "\n",
    "    print(f\"Iniciando conversão de colunas com padrões inteiros/decimais\")\n",
    "\n",
    "    # Regex para identificar valores inteiras e monetários\n",
    "    regex_data = re.compile(r\"^[a-zA-Zçãéê]+-feira, \\d{1,2} de [a-zA-Z]+ de \\d{4}$|^[a-zA-Zçãéê]+, \\d{1,2} de [a-zA-Z]+ de \\d{4}$\")\n",
    "\n",
    "    meses = {\n",
    "    'janeiro': '01', 'fevereiro': '02', 'março': '03', 'abril': '04',\n",
    "    'maio': '05', 'junho': '06', 'julho': '07', 'agosto': '08',\n",
    "    'setembro': '09', 'outubro': '10', 'novembro': '11', 'dezembro': '12'\n",
    "    }\n",
    "\n",
    "    def converter_data(data_str):\n",
    "        try:\n",
    "            # Remove o dia da semana e o hífen\n",
    "            partes = data_str.split(', ')\n",
    "            if len(partes) < 2:\n",
    "                return None\n",
    "            data_pt = partes[1]  # Ex: '11 de agosto de 2024'\n",
    "            partes_data = data_pt.split(' de ')\n",
    "            if len(partes_data) != 3:\n",
    "                return None\n",
    "            dia = partes_data[0].zfill(2)\n",
    "            mes = meses.get(partes_data[1].lower(), '01')\n",
    "            ano = partes_data[2]\n",
    "            # Formata para yyyy-MM-dd\n",
    "            data_formatada = f\"{ano}-{mes}-{dia}\"\n",
    "            return data_formatada\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Registrar a UDF\n",
    "    converter_data_udf = udf(converter_data, StringType())\n",
    "\n",
    "    # Obtendo colunas de tipo string\n",
    "    colunas_string = [coluna for coluna, dtype in df.dtypes if dtype == \"string\"]\n",
    "    colunas_data = []\n",
    "\n",
    "    # Identifica colunas com valores inteiras e monetários\n",
    "    for coluna in colunas_string:\n",
    "        df_sem_nulos = df.filter(col(coluna).isNotNull())\n",
    "        valores_amostra = df_sem_nulos.select(coluna).rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        if any(bool(regex_data.match(str(valor))) for valor in valores_amostra):\n",
    "            print(f\"\\nA coluna '{coluna}' contém valores no formato data.\")\n",
    "            colunas_data.append(coluna)\n",
    "        else:\n",
    "            print(f\"\\nA coluna '{coluna}' não contém valores no formato data.\\n\")\n",
    "\n",
    "    for coluna in colunas_data:\n",
    "\n",
    "        # Supondo que seu DataFrame se chama df e a coluna de data é 'data_str'\n",
    "        # 1. Crie a coluna formatada para yyyy-MM-dd\n",
    "        df = df.withColumn(coluna, converter_data_udf(df[coluna]))\n",
    "\n",
    "        # 2. Converta para tipo date e depois para dd/MM/yyyy\n",
    "        df = df.withColumn(coluna, to_date(df[coluna], 'yyyy-MM-dd'))\n",
    "\n",
    "        print(f\"STATUS OK - Coluna {coluna} convertida com sucesso para tipo 'date'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_data = define_data_columns(df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a9b8ec-c5b7-40b0-ac4f-e3a9572d5d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_with_hyphen(df):\n",
    "    \n",
    "    print(f\"Iniciando correção de valores nulos ou corrompidos em colunas strings\")\n",
    "    \n",
    "    # Identificar colunas strings (inteiras e decimais)\n",
    "    string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, (StringType))]\n",
    "\n",
    "    print(f\"\\nColunas strings identificadas: {string_cols}\")\n",
    "\n",
    "    # Contar valores nulos antes da transformação\n",
    "    null_counts_before = df.select([count(when(col(c).isNull(), c)).alias(c) for c in string_cols]).collect()[0].asDict()\n",
    "    print(f\"\\nValores nulos antes da transformação: {null_counts_before}\")\n",
    "\n",
    "    # Substituir valores nulos ou '#REF!' por '-' nas colunas strings\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull() | (col(col_name) == '#REF!'), '-').otherwise(col(col_name))\n",
    "        )\n",
    "        print(f\"\\nValor nulo ou '#REF!' na coluna {col_name} alterado para '-'\")\n",
    "\n",
    "    # Contar valores nulos e '#REF!' depois da transformação\n",
    "    null_or_ref_counts_after = df.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == '#REF!'), c))).alias(c) for c in string_cols\n",
    "    ]).collect()[0].asDict()\n",
    "    print(f\"\\nValores nulos ou '#REF!' depois da transformação: {null_or_ref_counts_after}\\n\")\n",
    "\n",
    "    # Verificar se todas as colunas tiveram seus valores nulos e '#REF!' substituídos\n",
    "    for col_name in string_cols:\n",
    "        if null_or_ref_counts_after[col_name] == 0:\n",
    "            print(f\"STATUS OK - Coluna {col_name} foi corretamente preenchida.\")\n",
    "        else:\n",
    "            print(f\"STATUS ALERTA - Coluna {col_name} ainda contém valores nulos ou '#REF!'!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_no_null = replace_with_hyphen(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1045fd8-2cfa-4564-9c86-9cc965d27b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def carregando_tabela_silver(df,delta_table_path):\n",
    "\n",
    "    print(f\"Iniciando o salvamento do DataFrame no formato Delta em: {delta_table_path}\")\n",
    "\n",
    "    try:\n",
    "        # 1- Obter a contagem de linhas ANTES de salvar ---\n",
    "        num_rows_to_save = df.count()\n",
    "        print(f\"Número de linhas no DataFrame a ser salvo: {num_rows_to_save}\")\n",
    "\n",
    "        # 2- Salvar o DataFrame no formato Delta ---\n",
    "        df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .partitionBy(\"data_ref\") \\\n",
    "                        .save(delta_table_path)\n",
    "\n",
    "        print(f\"DataFrame salvo com sucesso como tabela Delta particionada por 'extract' em: {delta_table_path}\")\n",
    "\n",
    "        # Início das Verificações de Qualidade Pós-Gravação \n",
    "\n",
    "        # 3- Garantir que os dados foram salvos no caminho\n",
    "        print(f\"\\n--- Verificação: Leitura da Tabela Delta Salva ---\")\n",
    "        df_delta_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        print(\"Esquema da tabela Delta lida:\")\n",
    "        df_delta_read.printSchema()\n",
    "        num_rows_saved = df_delta_read.count()\n",
    "\n",
    "        if df_delta_read.isEmpty():\n",
    "            print(f\"STATUS ALERTA - A tabela Delta salva em '{delta_table_path}' está vazia ou não pôde ser lida.\")\n",
    "        else:\n",
    "            print(f\"STATUS OK - A tabela Delta foi lida com sucesso de '{delta_table_path}' com {num_rows_saved} linhas recarregadas.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral ao salvar ou verificar a tabela Delta: {e}\")\n",
    "\n",
    "\n",
    "# Caminho para a external location do diretório silver\n",
    "silver_path = f\"/Volumes/grainlogistic/grain/silver\"\n",
    "\n",
    "# Sobreescrevendo dados particionados no diretório silver\n",
    "carregando_tabela_silver(df_no_null, silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
